# Voice AI Orchestration

A real-time voice agent powered by **LiveKit** with **RAG** (Retrieval-Augmented Generation) over uploaded documents. Talk to the agent via WebRTC, upload documents to build a knowledge base, and tweak the system prompt â€” all through a modern React UI with live streaming transcripts.

## Features

- ğŸ¤ **Real-time voice conversations** â€” WebRTC via LiveKit with low-latency audio
- ğŸ“„ **Document upload** â€” upload PDFs, TXT, MD, DOCX to create a knowledge base
- ğŸ§  **RAG-powered answers** â€” agent retrieves context from uploaded docs and speaks the answer
- âœï¸ **Editable system prompt** â€” customize the agent's personality and instructions live
- ğŸ“œ **Streaming transcript** â€” agent responses appear word-by-word in real time
- ğŸ” **RAG sources panel** â€” see which document chunks were used to answer
- ğŸ”‡ **Smart query filtering** â€” agent only queries KB for real questions, skips casual chatter
- ğŸ”„ **Session cleanup** â€” Pinecone index and uploads are cleared on each server restart

## Architecture

```
User Voice â†’ Deepgram STT â†’ Smart KB Filter â†’ RAG Lookup (Pinecone) â†’ OpenAI GPT-4o â†’ Cartesia TTS â†’ Audio Output
                                                                            â†•
                                                              Streaming Transcript â†’ Frontend
```

| Component | Technology |
|---|---|
| Voice / WebRTC | LiveKit |
| STT | Deepgram Nova-2 |
| LLM (Voice) | OpenAI GPT-4o (via LiveKit Agents) |
| LLM (RAG Synthesis) | Cerebras Llama-3.3-70b |
| TTS | Cartesia |
| Embeddings | Google Gemini (`gemini-embedding-001`, 3072-dim) |
| Vector Store | Pinecone (serverless) |
| Backend | FastAPI + Python |
| Frontend | React + Vite + TypeScript |

## Quick Start

### Prerequisites
- Python 3.11+
- [uv](https://astral.sh/uv) package manager
- Node.js 18+ and npm
- API keys (see below)

### 1. Clone and Setup

```bash
git clone <repo-url>
cd voice-ai-orch
```

### 2. Environment Variables

Copy `.env.example` to `.env` and fill in your keys:

```bash
cp .env.example .env
```

Required API keys:

| Key | Service | Get it from |
|---|---|---|
| `LIVEKIT_URL` | LiveKit (WebRTC) | https://cloud.livekit.io |
| `LIVEKIT_API_KEY` | LiveKit | Project settings |
| `LIVEKIT_API_SECRET` | LiveKit | Project settings |
| `OPENAI_API_KEY` | Voice LLM (GPT-4o) | https://platform.openai.com/api-keys |
| `DEEPGRAM_API_KEY` | STT | https://console.deepgram.com |
| `CARTESIA_API_KEY` | TTS | https://play.cartesia.ai |
| `GOOGLE_API_KEY` | Embeddings | https://aistudio.google.com/apikey |
| `CEREBRAS_API_KEY` | RAG LLM | https://cloud.cerebras.ai |
| `PINECONE_API_KEY` | Vector Store | https://app.pinecone.io |

### 3. Install Backend Dependencies

```bash
uv sync
```

### 4. Start All Services (3 terminals)

**Terminal 1 â€” API Server** (http://localhost:8000):
```bash
uv run api_server.py
```

**Terminal 2 â€” LiveKit Voice Agent:**
```bash
uv run agent.py dev
```

**Terminal 3 â€” Frontend** (http://localhost:5173):
```bash
cd frontend
npm install   # first time only
npm run dev
```

### 5. Use It

1. Open **http://localhost:5173**
2. **Upload** a PDF or text document (top-left panel)
3. **Edit the prompt** if desired (top-right panel)
4. Click **Start Call** and speak
5. Ask a question about the uploaded document
6. Watch the streaming transcript + RAG sources appear in real time

## Project Structure

```
voice-ai-orch/
â”œâ”€â”€ agent.py                    # Entry point for LiveKit CLI
â”œâ”€â”€ api_server.py               # FastAPI backend (upload, prompt, token, agent control)
â”œâ”€â”€ initialize_rag.py           # One-time RAG initialization script
â”œâ”€â”€ conversation_logger.py      # Conversation transcript logger
â”œâ”€â”€ voice_agent/                # Voice agent package
â”‚   â”œâ”€â”€ __init__.py             # Exports entrypoint()
â”‚   â”œâ”€â”€ pipeline.py             # Component factories + pipeline assembly
â”‚   â”œâ”€â”€ voice_assistant.py      # VoiceAssistant agent (RAG + prompt orchestration)
â”‚   â””â”€â”€ prompt_manager.py       # 3-layer prompt system (core + persona + RAG context)
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ rag_engine.py           # LlamaIndex + Pinecone RAG engine
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx             # Main app with streaming transcript state
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ VoiceRoom.tsx   # LiveKit room + useVoiceAssistant streaming
â”‚   â”‚   â”‚   â”œâ”€â”€ PromptEditor.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ DocUpload.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Transcript.tsx
â”‚   â”‚   â”‚   â””â”€â”€ RagSources.tsx
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â””â”€â”€ client.ts       # Backend API client
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ Prompt/
â”‚   â””â”€â”€ prompt.md               # System prompt file
â”œâ”€â”€ pyproject.toml              # Python dependencies (managed by uv)
â””â”€â”€ .env.example                # Environment variable template
```

## API Endpoints

| Method | Endpoint | Description |
|---|---|---|
| `POST` | `/api/token` | Generate LiveKit access token |
| `POST` | `/api/upload` | Upload document to KB |
| `GET` | `/api/documents` | List uploaded documents |
| `DELETE` | `/api/documents/{id}` | Remove document from KB |
| `GET` | `/api/prompt` | Get system prompt |
| `PUT` | `/api/prompt` | Update system prompt |
| `GET` | `/api/agent/status` | Agent running status |
| `POST` | `/api/agent/start` | Start voice agent |
| `POST` | `/api/agent/stop` | Stop voice agent |
| `GET` | `/api/transcripts` | List conversation transcripts |
| `GET` | `/api/transcripts/{filename}` | Get transcript content |

## Key Design Decisions

- **Session isolation** â€” Pinecone index clears on server restart so each session starts fresh
- **Smart KB filtering** â€” casual messages ("thanks", "okay") skip the RAG pipeline to avoid irrelevant lookups
- **Streaming transcripts** â€” uses LiveKit's `useVoiceAssistant` hook for word-by-word agent text display
- **3-layer prompt system** â€” core directives (brevity, voice format) + user persona + dynamic RAG context
- **Compact RAG mode** â€” single LLM call for synthesis to keep latency under real-time constraints

## Known Limitations

- Single-room design â€” one voice call at a time
- Pinecone free tier has index limits
- Index is cleared on every server restart (by design for session isolation)

## License

MIT
